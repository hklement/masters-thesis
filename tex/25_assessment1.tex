\section{Assessment}\label{section:assessment1}

As discussed in Section~\ref{section:practical-programming}, practical programming exercises are an important component of \cs education. According to Pieterse~\cite{pieterse2013automated}, offering assignments as opportunities to practice is essential for students in order to develop programming skills. However, assignments are considerably more valuable if fast and accurate feedback is supplied. Assessment allows providing both learners and teachers with feedback about the learning process and is essential to guide students' learning~\cite{ihantola2010review}.

Traditionally, teachers perform assessment manually. However, manual assessment of code submissions is a time-consuming and error-prone activity that involves a time delay before learners receive feedback. Beyond that, educators' time is limited and may be spent better on other aspects than assessing students' work.

Student evaluation is particularly a challenge for courses with many participants. While manual grading might be realizable for on-campus courses using a sufficient number of teaching assistants, huge student numbers in large-scale e-learning environments make manual grading performed by the teaching team infeasible~\cite{shah2014some}. \moocs, which aim at providing unlimited participation, face a scalability problem in this regard~\cite{rogers2014acce}. In order to provide assessment to an open-ended number of learners, \moocs have to employ scalable assessment approaches, such as automated assessment and peer assessment.

\subsection{Automated Assessment}\label{subsection:automatic-assessment}

Automated assessment refers to assessment approaches that are solely based on automatable workflows and automatically collectable properties. Automated techniques have been used for the assessment of programming assignments almost as long as programming has been taught~\cite{pieterse2013automated}.

\subsubsection{Advantages}

On campus, automated assessment approaches are used to keep teachers' workload within reasonable limits despite growing students numbers~\cite{vogel2014quality}. This way, the time required for assessment activities can be cut down without reducing quantity and quality of practical exercises. Furthermore, the amount of time that instructors can spend on mentoring and supporting students is increased~\cite{vihavainen2013scaffolding}.

Automated program evaluation can also be beneficial to students. While human graders and especially teams of multiple graders usually judge subjectively and inconsistently, automated assessment can provide objective and consistent evaluation~\cite{ala2005survey}. Furthermore, students are provided with immediate feedback, which is an important benefit in programming education. Receiving instant feedback is particularly useful for novice programmers since misconceptions are uncovered as early as possible~\cite{vujovsevic2013software}. The concept of providing feedback at any time and any place applies notably well to virtual courses~\cite{malmi2002experiences}, such as \moocs, and provides learners a unique advantage~\cite{chauhan2014massive}. Since assessment resources are virtually unrestricted, automated grading allows students to increase mastery by iteratively improving and resubmitting their homework~\cite{fox2014software}.

\subsubsection{Evolution}

Systems that automatically assess students' programming assignments have been designed and used for over fifty years. Systematic overviews of assessment systems' approaches and capabilities have been published by Ala-Mutka~\cite{ala2005survey}, Douce et al.~\cite{douce2005automatic}, and Ihantola et al.~\cite{ihantola2010review}.

Douce et al. present a historical overview of automated assessment systems that focuses on systems that are based on executing tests in an automated fashion. The authors classify these systems into three generations.

The first generation covers the initial attempts to automate the assessment of programming assignments. In general, first-generation systems were specifically tailored solutions that required modifications to compilers and \glspl{os}, demanded a great deal of expertise, and were limited to the usage in their particular setting. The very first system has been described by Hollingsworth~\cite{hollingsworth1960automatic}. Its purpose was to evaluate programs written in assembly language, which had to be handed in on punched cards. The system was not only useful for saving teacher resources but also for allocating computing resources, which were severely limited at that time.

The second generation of automated assessment systems is characterized by the adoption of automated tools and utilities, provided by increasingly advanced \glspl{os} and tool sets. The systems could be operated by instructors and students using a \gls{cli} or a \gls{gui}. Second-generation systems introduced more sophisticated assessment strategies involving multiple assessable properties, such as correctness, efficiency, and style. Several systems also include management capabilities for courses and assignments. Well-known representatives of second-generation systems are ASSYST~\cite{jackson1997grading}, BOSS~\cite{joy2005boss}, and Ceilidh~\cite{benford1995ceilidh}.

Third-generation automated assessment systems took advantage of advancing web technologies. They comprise features such as web interfaces, increasingly sophisticated testing approaches, interactive feedback, richer content management features, and plagiarism detection. Systems of the third generation include instances of second-generation systems that continued to develop, such as BOSS, and successors of former systems, such as CourseMarker~\cite{higgins2003coursemarker}, which evolved from Ceilidh.

While Douce et al. assign state-of-the-art automated assessment systems to the third generation, their work cannot cover trends that emerged after 2005, for instance the growing demand for practical assignments in e-learning. In that respect, Ihantola et al.~\cite{ihantola2010review} report an increasing interest in extending \glspl{lms} with automated assessment capabilities in order to fit the special needs of \cs education better. For the same reason, programming \moocs should be provided with modern capabilities for automatic code assessment.

\subsubsection{Design Challenges}

High-quality assignments are seen as a vital part of a successful course~\cite{feldman1996quest}. While manual assessment allows compensating for poor assignment design, the use of automated assessment techniques increases the need for carefully designed assignments~\cite{pieterse2013automated}. The creation of automatically assessable programming assignments is considered a challenging task that requires special attention~\cite{ala2005survey}.

Whereas automated assessment saves instructors' time by outsourcing formerly manually performed grading activities, a considerable amount of the gained time should be allocated for designing and implementing resources for automated assessment. While efforts may only be shifted from grading activities to design activities for small class sizes, the trade-off increasingly shows its strengths with rising student numbers.

Whenever assessment is performed without human intervention, the assignment specification should be provided as unambiguous as possible. Ambiguous specifications permit different interpretations, which can lead to technically valid student solutions being rejected by an automatic grader. Within programming, interpretation is key to success, which is why assignment instructions must guide interpretation precisely for successful automated assessment~\cite{douce2005automatic}. In contrast, careless formulation of assessment criteria can result in improper assessment~\cite{pieterse2013automated}. Therefore, ambiguity must be minimized in order to increase fairness and quality of assessment~\cite{renz2014handling}. Cerioli and Cinelli~\cite{cerioli2008grasp} even regard an extremely precise problem specification, which allows a completely predictable behavior of implementations, as a prerequisite for automated grading based on functional correctness. However, a reasonable balance between the risk of misinterpretation and excessive detail has to be found because wordier specifications, which point out every detail, can result in trivial assignments lacking any demand to reason about the problem~\cite{renz2014handling}.

Besides addressing the problem of ambiguity, the definition of pedagogically sound test cases is a time-consuming activity~\cite{cerioli2008grasp} that requires both expertise and experience~\cite{vihavainen2012multi}. Pieterse~\cite{pieterse2013automated} names test data ``the Achilles' heel of any system that applies automated assessment of programming assignments''. In order to enable accurate assessment and prevent incorrect solutions from passing the evaluation, tests must be designed well. Otherwise, learners might submit deficient solutions but remain unaware of their incorrectness.

\subsubsection{Approaches}

There are several approaches for performing automated assessment of programming assignments. They can be split up into dynamic approaches, which require execution of the program under test, and static approaches, which do not. While most approaches focus on evaluating the functional completeness and correctness of a program, others aim at evaluating aspects of quality and style.

\paragraph{I/O-based Assessment}

\Gls{io}-based assessment refers to assessing a program solely by using a standard \gls{io} interface. The program under test is supplied with predefined values and is verified to produce expected output values.

The advantage of this approach is its versatility. \Gls{io}-based assessment can be applied to any program using an \gls{io} interface and to any programming language that can be executed on the same test environment~\cite{ihantola2010review}. Moreover, test cases may be reused across multiple languages since a universal interface is sufficient for their execution.

A shortcoming of the approach is that it may fail to give an appropriate mark if a student program's output does not exactly match the expected format~\cite{pieterse2013automated}. Therefore, \gls{io}-based assessment techniques are not usable if strict format requirements are not feasible or if freedom in formatting should be allowed. However, implementing \gls{io} handling that is robust to irrelevant output differences, such as whitespace and orthographic mistakes, is a challenge~\cite{douce2005automatic}.

Due to lacking insights into the inner mechanics of a code submission, \gls{io}-based assessment is limited to testing side effects that are exposed in the form of program output. For the same reason, \gls{io}-based assessment is not qualified for providing the learner with feedback regarding why her submission deviates from the specification.

\paragraph{Assessment Using Industrial Testing Tools}

In-depth feedback can be provided by utilizing industrial-strength testing tools and frameworks. Such tools are widely used, are actively developed, and can supply deeper insights into the program under test. Since testing is an established practice in industry, myriads of testing frameworks exist for virtually every programming language and application domain.

Ihantola et al.~\cite{ihantola2010review} name three classes of industrial testing tools that are used by automated assessment systems: xUnit-based frameworks, acceptance testing frameworks, and web testing frameworks.

xUnit is a collective term for numerous testing frameworks that derive their design from SUnit~\cite{beck1994simple}, an influential testing framework for Smalltalk, which is considered ``the mother of all unit testing frameworks''~\cite{ducasse2003sunit}. Widespread xUnit derivatives include CUnit\foo{http://cunit.sourceforge.net/} for C, HUnit\foo{http://hunit.sourceforge.net/} for Haskell\foo{https://www.haskell.org/}, and JUnit\foo{http://junit.org/} for Java. These language-specific testing frameworks enable assessment techniques that can evaluate the functionality of entities smaller than a complete program, such as single classes, methods, and even statements~\cite{ala2005survey}.

Acceptance testing refers to an industrial testing technique that is based on customers specifying test scenarios that have to be passed so that user stories are considered to be correctly implemented. This testing approach helps customers and developers to foster a common understanding of how software under development should work once it is finished. Acceptance testing frameworks, such as Cucumber\foo{http://cukes.info/}, FitNesse\foo{http://www.fitnesse.org/}, and Lettuce\foo{http://lettuce.it/}, usually rely on easily understandable plain-text \glspl{dsl}, similar to natural language. This allows non-technical stakeholders to contribute their domain knowledge by providing scenarios that specify navigation through the application, inputs to the application, and expected outputs~\cite{fox2012crossing}. Scenarios are turned into executable tests whose successful execution is to be achieved. When used for student assessment, acceptance testing offers the advantage that a single specification can serve as both assessment basis and exercise instructions since it is given in easily understandable form and expected to be complete.

Web testing frameworks, such as Selenium\foo{http://www.seleniumhq.org/} and Watir\foo{http://watir.com/}, are useful tools for assessing web application exercises. Instead of accessing low-level \glspl{api}, web testing tools test web applications using their public web interfaces. This can either be done by controlling a real web browser in an automated fashion or by simulating a web browser by means of \gls{http} requests.

\paragraph{Assessment of Testing Skills}

Modern software development processes, such as Scrum~\cite{schwaber1997scrum} and \gls{xp}~\cite{beck2000extreme}, promote test-first practices, which help to discover design flaws as early as possible in the development cycle and underline the value of regression tests for continuous delivery.

When novice programmers are assessed using traditional automated approaches, they are neither encouraged nor rewarded for performing testing themselves since an automated grader verifies their programs' correctness anyhow. As a result, learners might not reflect upon the behavior of their code, but they might solely focus on providing a solution that satisfies the automated approach~\cite{edwards2003improving}. However, efficient automatic testing approaches should not invite students to get careless. Instead, students should learn to design and test their programs thoroughly before submitting them~\cite{ala2005survey}. In this sense, Edwards~\cite{edwards2003improving} argues that students need to acquire software testing skills. He suggests exposing students to \gls{tdd}, so that they perform more testing and eventually appreciate its value for the development process. Moreover, a testing-oriented assessment approach empowers students with the responsibility of demonstrating their own programs' correctness and validity. As a result, the learning experience is enhanced and learners produce higher-quality code. According to Pieterse~\cite{pieterse2013automated}, the application of test-based assessment combined with training in software testing can provide a learning experience where students learn to favor robust and precise solutions over improvised ones.

The term meta testing refers to a test-based assessment approach that evaluates students' software testing skills. Instead of providing learners with prepared tests, be it explicitly as visible part of an exercise or implicitly as the basis for program evaluation, this approach demands learners to write tests themselves. They are required to submit working program code along with proper tests. Grading can be based on judging the extent to which the student-written code fulfills the accompanying tests, the tests' level of quality, and the fraction of code covered by tests. Additionally, the teacher might incorporate her own tests into the assessment in order to validate that the student's submission indeed satisfies the exercise specification.

\paragraph{Assessment of GUI Applications}

Even though focusing solely on \gls{cli} applications may be perfectly sufficient for conveying programming skills, such an educational approach may be seen as uninspiring by learners to whom graphical applications are familiar and much more attractive than \gls{cli}-based ones~\cite{douce2005automatic}. Instead, students are interested in learning how to build programs with \glspl{gui}~\cite{english2004automated}. Likewise, applications that produce animations or perform 3D rendering are usually appealing to learners.

However, \gls{gui} applications are difficult to assess since \gls{io} redirection, as used for the assessment of \gls{cli} applications, is not applicable. Developing software tests for programs involving significant \glspl{gui} is ranked beyond the typical abilities of students and educators~\cite{thornton2008supporting}.

A response to this problem are educational \gls{gui} libraries, such as presented by English~\cite{english2004automated} and Thornton et al.~\cite{thornton2008supporting}. These libraries are designed for novice programmers and provide built-in means for automated testing and assessment. Moreover, the latter framework is explicitly aimed at allowing students to write tests themselves. Therefore, it can enable automated assessment for \gls{gui} applications that follows a \gls{tdd}-based assessment approach.

The edX course ``Foundations of Computer Graphics''\foo{https://www.edx.org/course/foundations-computer-graphics-uc-berkeleyx-cs-184-1x} provides scalable assessment of graphically sophisticated student programs, based on image comparison. Visual output generated by a learner's program is exported to an image file, uploaded by the learner, and automatically compared with a reference image. In order to be considered a correct solution, the output of the learner's program must not vary considerably from a reference image. The degree of permitted deviation is controlled using a threshold of pixels that are allowed to differ.

\paragraph{Assessment of Style}

Besides functional completeness and correctness, there are further aspects that are crucial to the quality of software, such as its complexity, extensibility, and maintainability.

Writing code in good style is important because program code is read much more often than it is written~\cite{rogers2014acce}. Since software projects are usually carried out in groups, developers need to follow established coding conventions that facilitate a common understanding among them and guarantee a certain degree of quality. In general, good coding style promotes readability, absence of errors, security, extensibility, and modularity~\cite{rogers2014acce}.

However, novice programmers are reported to commonly perceive programming style as less significant~\cite{ala2004supporting} and to have little appreciation for best practices, which are required for successful long-term multi-person programming projects~\cite{xiao2014multiplayer}. Therefore, programming style is an important issue to teach beginning programmers. It is often neglected in education, though~\cite{ala2004supporting}.

Automated techniques can help to involve programming style into the assessment process. In contrast to functionality, which is usually assessed by executing a program submission, properties of style are typically collected using static evaluation approaches.

A common practice for judging a student program's quality is detecting so-called code smells, such as unused variables, redundant logical expressions, and implicit constants~\cite{truong2005learning}. Furthermore, automated style evaluation can examine programs' adherence to given coding guidelines in terms of indentation size, mandatory source code documentation, and more~\cite{ala2004supporting}. High-complexity program submissions can be detected by employing software metrics, such as Halstead's complexity measures~\cite{halstead1977elements} and McCabe's cyclomatic complexity~\cite{mccabe1976complexity}, and by comparing their structure to that of a model solution~\cite{truong2005learning}.

However, there are program characteristics that are hard to assess automatically, for instance quality of comments, meaningfulness of variable names, and adherence to good practices, such as the Single Responsibility Principle~\cite{martin2003agile}. Evaluating such subtle or complex software properties requires the trained eye of a human assessor.

\subsection{Peer Assessment}

A different approach towards scalable assessment is peer assessment. Instead of relying on automation, peer assessment involves learners into the assessment process. Students' submissions are graded by peers who take the same course, usually as part of mandatory course tasks. Since learners cannot provide expert grading, every student solution is typically graded by three to seven peers. The final grade is determined by aggregating the individual scores using basic statistical means or more sophisticated probabilistic models~\cite{piech2013tuned}.

Peer assessment is a popular choice for assessing subjective topics and complex problems for which accurate machine grading is difficult to provide~\cite{shah2014some}. With this in mind, peer assessment seems to be a qualified means for assessing subtle properties of programming assignments. Peer assessment for programming assignments is based on the philosophy that learning to program should be a social experience emphasizing human-human interaction~\cite{lahtinen2005study}. In this sense, it is similar to code review among colleagues, which is a common procedure for quality assurance in industrial software engineering and open-source software development.

Peer assessment can overcome some limitations that apply to automated assessment. Automatic approaches can only evaluate clearly defined problems with specified interfaces. They cannot award points for creative solutions, though. Peer assessment can be a valuable tool for issues that are hardly automatically testable~\cite{safran2008collaborative}. While some quality metrics can be assessed automatically, subtle software properties, such as the qualification of a chosen approach, the elegance of an implementation, portability, and reusability, are more difficult to quantify~\cite{douce2005automatic}. Peer assessment also offers the possibility to provide more and better feedback for individual student solutions~\cite{safran2008collaborative}.

The requirement for learners to grade their peers' program submissions should not be seen as a burdensome duty but as an opportunity. Seeing fellow students' approaches to the same programming problems may provide new ideas and reveal different paths to a viable solution~\cite{hamalainen2009use,reily2009two}. Moreover, students who are familiar with peer assessment might improve the readability of their programs since they are reviewed manually~\cite{hamalainen2009use}. Lastly, giving peer reviews is reported to increase overall learning outcomes~\cite{reily2009two} and improve the learning process due to a deeper occupation with the subject~\cite{safran2008collaborative}.

However, peer assessment also has some shortcomings. The time it takes to receive quality feedback is much longer than with automated assessment techniques. Moreover, in contrast to objective results provided by automatic graders, peer assessment involves the potential for biased and inaccurate feedback. This applies especially to beginners' courses where the majority of students are struggling with the course content~\cite{singh2013automated}.
