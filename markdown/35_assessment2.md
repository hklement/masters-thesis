## Assessment {#section:assessment2}

Section \ref{section:assessment1} underlined the relevance of assessment for the learning process and presented multiple approaches towards scalable assessment of programming assignments. Due to its focus on large-scale e-learning environments, such as \moocs, scalable assessment is a crucial requirement of \tool.

### Approach

Since \tool should provide an appropriate platform for teaching programming to everyone, including complete beginners, we decided to rely on automated assessment techniques rather than peer assessment. Automated assessment provides highly available and objective evaluation which makes it a predestined approach to supply learners with means for step-by-step refinement of their solutions.

Integration of additional manual assessment capabilities, however, is not within the scope of \tool. Firstly, manual assessment is infeasible for large numbers of learners and cannot provide general value. Secondly, a coexistence of automatic and manual assessment can easily create confusion among both teachers and learners if the origins of grades are not completely transparent \cite{ihantola2010review}.

We decided not to dictate a universally applicable assessment approach, such as \gls{io}-based testing, but to grant instructors the freedom to select an assessment strategy that they consider appropriate for a specific use case. Besides being used for running students’ submissions, Docker can provide a versatile platform for executing tests for assessment. In this way, instructors are very flexible in their choice of a particular assessment strategy. For instance, teachers are free to use an arbitrary testing framework, such as the one that is best practice for the language they teach, the one that fits the application domain best, or the one that they are most experienced with.

Besides relying on industry-strength testing tools, instructors can also choose to utilize tailor-made scripts for their assessment workflows. However, we encourage instructors to favor well-known solutions over improvised ones since established testing frameworks usually supply greater functionality and robustness, and can provide learners with relevant experience in using them.

Since the system’s underlying assessment approach is visible to learners, they get in contact with the concept of software testing from the very beginning. Besides imparting an objective quality to the assessment, test-based evaluation provides learners with an understanding of the primary method for verifying industrial software \cite{douce2005automatic}. Moreover, learners become accustomed to the idea of software testing as a means for controlling software quality and might be more willing to write their own tests later \cite{vihavainen2013scaffolding}. Instructors are free to provide the tests they use for assessment as visible part of the exercise skeleton, in this way permitting even deeper insights into the testing approaches used by professional developers.

### Feedback

To take full advantage of the role of assessment as a feedback channel for learners, the results of teacher-provided test cases should convey learners a good understanding regarding the extent to which their code adheres to the exercise specification. Just as error messages provided by programming language interpreters can be too cryptic for novice programmers (see Section \ref{section:hint-generation1}), the output generated by a testing framework can be confusing for beginners \cite{lahtinen2005study}. Inexperienced programmers might find it difficult to match the feedback supplied with a failing test to errors in their code \cite{singh2013automated}.

In order to facilitate learners’ troubleshooting, we want to supplement test frameworks’ low-level output with instructor-provided feedback that is more easily understandable. Analogous to providing hint messages for common program mistakes, teachers are encouraged to provide understandable natural-language feedback for every test. Consequently, in the case of a failing test, the learner is supplied with a useful hint on how the program’s behavior does not fulfill the specification and how it can be improved. Provided with a clear understanding of her program’s inadequate aspects, the student might be more motivated to revise her solution.

### Scoring

In order to map the results of a test run to a numerical grade, \tool calculates a score that is based on the ratio of passed tests to failed tests. Since practical programming assignments are provided as a service to external e-learning applications, it remains in their responsibility to incorporate \tool’s scores into their grading practices. Scores might directly be mapped to a number of points or might be used for a threshold-based binary decision whether the learner has succeeded or not.

In order to control the influence of particular test cases on the final score, instructors should be enabled to assign different weights to tests. This way, a teacher can emphasize the role of a test covering a challenging program aspect by awarding more points for it.